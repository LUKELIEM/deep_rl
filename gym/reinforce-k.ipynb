{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE-K\n",
    "\n",
    "We will implement a simple but powerful REINFORCE agent in PyTorch that directly backprop based on the methodology used by Andrej Karpathy. We will bypass PyTorch's recommended method of using torch.distribution's score function to implement reinforcement learning.\n",
    "\n",
    "Wish me Luck!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.3\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "import gym\n",
    "import pickle as pickle\n",
    "import pympler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import gc\n",
    "\n",
    "# For memory tracking\n",
    "from pympler import summary\n",
    "from pympler import muppy\n",
    "\n",
    "cpu_dtype = torch.FloatTensor\n",
    "gpu_dtype = torch.cuda.FloatTensor\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE-Karpathy for \"Pong\"\n",
    "\n",
    "The following code uses the basic policy gradient method (REINFORCE) to train a 2 layer NN to play Pong.\n",
    "\n",
    "The time needed to train 10 episodes is 13 sec with Intel I7 (4-cores). The forward pass takes 4 seconds. So implementing this in PyTorch and using GPU will speed things up by 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import pickle as pickle\n",
    "import gym\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "\n",
    "learning_rate  = 1e-3\n",
    "epoch_reward_history=[]\n",
    "    \n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "  model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "  model = {}\n",
    "  model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "  model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "\n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "  I = I[35:195] # crop\n",
    "  I = I[::2,::2,0] # downsample by factor of 2\n",
    "  I[I == 144] = 0 # erase background (background type 1)\n",
    "  I[I == 109] = 0 # erase background (background type 2)\n",
    "  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "  return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "  h = np.dot(model['W1'], x)\n",
    "  h[h<0] = 0 # ReLU nonlinearity\n",
    "  logp = np.dot(model['W2'], h)\n",
    "  p = sigmoid(logp)\n",
    "  return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "  dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "  dh = np.outer(epdlogp, model['W2'])\n",
    "  dh[eph <= 0] = 0 # backpro prelu\n",
    "  dW1 = np.dot(dh.T, epx)\n",
    "  return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "verbose = False\n",
    "print_every = 20\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "forward_time=0\n",
    "backward_time=0\n",
    "other_time = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "while episode_number < 30000:\n",
    "  if render: env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  t1 = time.time()\n",
    "  aprob, h = policy_forward(x)\n",
    "  action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "  t2 = time.time()\n",
    "  forward_time += t2-t1\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) # observation\n",
    "  hs.append(h) # hidden state\n",
    "  y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  t5 = time.time()  \n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "\n",
    "  t6 = time.time()\n",
    "  other_time += t6-t5\n",
    "\n",
    "  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "  if done: # an episode finished\n",
    "    episode_number += 1\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    eph = np.vstack(hs)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    \n",
    "    t3 = time.time()    \n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    t4 = time.time()\n",
    "    backward_time += t4-t3    \n",
    "    \n",
    "    for k in model: \n",
    "        grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for k,v in model.items():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    if episode_number % print_every == 0:     \n",
    "        print ('Episode %d - reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n",
    "\n",
    "    epoch_reward_history.append([reward_sum, running_reward])\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n",
    "\n",
    "  if reward != 0 and verbose: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "    print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))\n",
    "    \n",
    "end = time.time()\n",
    "print (\"Time to complete\", end-start)\n",
    "print (\"Time to forward pass\", forward_time)\n",
    "print (\"Time to backward pass\", backward_time)\n",
    "print (\"Time to other stuffs\", other_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE-K\n",
    "\n",
    "After the great professor and mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        m.weight.data.normal_(0, math.sqrt(2. / fan_in))  # Xavier for ReLU\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# The function approximator of the Policy is a 2 layer NN. \n",
    "# - The policy takes in the state of Pong (a resampled 40x40 diff image of 2 successive frames )\n",
    "# - The action is the softmax output (Left or Right)\n",
    "# - there are 200 hidden units in the NN\n",
    "class Policy(nn.Module):\n",
    "            \n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(6400, 200)\n",
    "        self.affine2 = nn.Linear(200, 2)  \n",
    "        self.apply(weights_init)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.affine2(x)\n",
    "        return action_scores   # output logit\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro80(I):\n",
    "    \"\"\" \n",
    "    prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \n",
    "    \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "\n",
    "def estimate_Q(r, gamma):\n",
    "  \"\"\" \n",
    "  take 1D float array of rewards and compute discounted reward --> Estimating Q(s,a)\n",
    "  \"\"\"\n",
    "  r = np.array(r)  # convert list to numpy\n",
    "\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0:   # reset the sum, since this was a game boundary specific to Pong.\n",
    "        running_add = 0 \n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\" \n",
    "    Use Policy to select an action based on state returned by Pong. The output expected by the PONG \n",
    "    is:\n",
    "    ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
    "    \n",
    "    So we need to return 2 (UP/RIGHT) and 3(DOWN/LEFT)\n",
    "    \"\"\"\n",
    "    \n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    \n",
    "    # Use Policy to generate probability for action\n",
    "    logit = policy(Variable(state.type(gpu_dtype), requires_grad=False))\n",
    "\n",
    "    # Replace PyTorch recommended RL implementation\n",
    "    # Sample action stochastically    \n",
    "    # m = Categorical(probs)\n",
    "    # action = m.sample()\n",
    "\n",
    "    prob = F.softmax(logit, dim=1)\n",
    "    log_prob = F.log_softmax(logit, dim=1)\n",
    "    action = prob.multinomial().data   # action is sampled here\n",
    "    \n",
    "    if verbose:\n",
    "        print (\"Pong's state output:\", state.shape)\n",
    "        print (\"NN output:\", logit)\n",
    "        print (\"Pong Action:\", action)\n",
    "        print (\"log_prob(action):\", log_prob)\n",
    "        \n",
    "    log_prob = log_prob.gather(1, Variable(action))\n",
    "        \n",
    "    # Store log_prob (score function) into a list for calculating policy gradient    \n",
    "    policy.saved_log_probs.append(log_prob)\n",
    "    \n",
    "    if verbose:\n",
    "        print (\"Stacked log_prob:\", policy.saved_log_probs)\n",
    "    \n",
    "    return action\n",
    "\n",
    "def finish_batch():\n",
    "    \"\"\" \n",
    "    Based on REINFORCE, policy gradient is computed at the end of a batch (instead of an\n",
    "    episode). It is then used to update the Policy's weights\n",
    "    \"\"\"\n",
    "\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []   # This is v_t\n",
    "    \n",
    "    if verbose:    \n",
    "        print (\"Rewards:\", policy.rewards)\n",
    "            \n",
    "    # In the main loop, reward for each time step is stored in the list policy.rewards[].\n",
    "    # At the end of the episode, this is used to generate v_t for each time step.\n",
    "    for r in policy.rewards[::-1]:\n",
    "        if r != 0:  # reset the sum, since this was a game boundary (pong specific!)\n",
    "            R = 0\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "        \n",
    "    if verbose:    \n",
    "        print (\"Discounted Rewards:\", rewards)\n",
    "       \n",
    "    rewards = torch.Tensor(rewards)\n",
    "    \n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "\n",
    "    if verbose:\n",
    "        print (\"v_t:\", rewards)   \n",
    "\n",
    "    # Calculate policy gradient ∇_θ log (π_θ ( s_t , a_t ) )v_t\n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)   # negative because gradient descent\n",
    "\n",
    "    if verbose:\n",
    "        print (\"Policy Gradient:\", policy_loss)  \n",
    "        \n",
    "    optimizer.zero_grad()  # zero the gradients before running the optimizer\n",
    "    \n",
    "    # Sum policy gradients for all time steps in the episode\n",
    "    policy_loss = torch.cat(policy_loss).sum()   \n",
    "    if verbose:\n",
    "        print (\"Policy loss (after cat and sum):\", policy_loss)  \n",
    "        \n",
    "    # The TRICK: backward() on policy_loss instead of policy\n",
    "    # Policy's parameters are updated here.\n",
    "    policy_loss.backward()  \n",
    "    optimizer.step()\n",
    "    \n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast Reward: -10.00\tRunning Reward: -14.95\n",
      "Episode 10\tLast Reward: -14.00\tRunning Reward: -14.66\n",
      "Episode 20\tLast Reward: -12.00\tRunning Reward: -14.44\n",
      "Episode 30\tLast Reward: -14.00\tRunning Reward: -14.31\n",
      "Episode 40\tLast Reward: -10.00\tRunning Reward: -14.04\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "gamma=0.99\n",
    "render=True\n",
    "verbose=False  # To step through the code and understand what is going on\n",
    "log_interval=10\n",
    "update_interval = 800  # This is the number of game steps before we do a policy update\n",
    "lr = 1e-3 \n",
    "\n",
    "reinforce=[]\n",
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "# D_in is input dimension; H is hidden dimension; D_out is output dimension.\n",
    "D_in, H, D_out = 6400, 200, 2\n",
    "\n",
    "# Randomly initialize weights\n",
    "if torch.cuda.is_available():\n",
    "    W1 = torch.randn(D_in, H).type(gpu_dtype)/math.sqrt(2./D_in)\n",
    "    W2 = torch.randn(H, D_out).type(gpu_dtype)/math.sqrt(2./H)\n",
    "else:\n",
    "    W1 = torch.randn(D_in, H).type(cpu_dtype)/math.sqrt(2./D_in)\n",
    "    W2 = torch.randn(H, D_out).type(cpu_dtype)/math.sqrt(2./H) \n",
    "    \n",
    "\n",
    "        \n",
    "# Main loop\n",
    "prev_x = None\n",
    "running_reward = -15\n",
    "start = time.time()\n",
    "\n",
    "for i_episode in range(1000+1): # run 30000 episode    \n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    \n",
    "    # Clear out the stacked local gradients\n",
    "    eph = None\n",
    "    epdlogp = None\n",
    "    epx = None\n",
    "    epr = None\n",
    "    rewards = []\n",
    "    \n",
    "    for t in range(10000):  # Don't infinite loop while learning\n",
    "        \n",
    "        # Downsample 210x160x3 frame into 6400 (80x80) 1D float vector\n",
    "        cur_x = prepro80(state)\n",
    "        state = cur_x - prev_x if prev_x is not None else np.zeros(6400)\n",
    "        prev_x = cur_x\n",
    "        \n",
    "        \"\"\"\n",
    "        Sample an action from Policy based on state provided by env\n",
    "        (Entering Torch(GPU/CPU) Land!!!)\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)   # state: numpy-->Tensor\n",
    "\n",
    "        # 2-layer Neural Network - Forward Pass\n",
    "        #  Input - state\n",
    "        #  Output - logp\n",
    "    \n",
    "        # Stage 1 - Matrix Multiplication                               #[1]    \n",
    "        h = state.mm(W1)\n",
    "        # Stage 2 - ReLU                                                #[2]    \n",
    "        h_relu = h.clamp(min=0)\n",
    "        # Stage 3 - Matrix Multiplication                               #[3]    \n",
    "        logp = h_relu.mm(W2)\n",
    "\n",
    "        # A Pong action is sampled from the Softmax Classifier\n",
    "        prob = F.softmax(Variable(logp), dim=1)\n",
    "        sample = prob.multinomial().data  \n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Softmax prob:\", prob)\n",
    "            print(\"Sampled output:\", sample)\n",
    "        \"\"\"\n",
    "        Stack local gradients for policy update at the end of episode/batch\n",
    "        \"\"\"\n",
    "        \n",
    "        y = torch.zeros(1, D_out).scatter_(1,torch.LongTensor(sample) ,1)\n",
    "        dlogp = y - prob.data\n",
    "\n",
    "        epdlogp =  torch.cat((epdlogp, dlogp), 0) if epdlogp is not None else dlogp\n",
    "        eph = torch.cat((eph, h), 0) if eph is not None else h\n",
    "        epx = torch.cat((epx, state), 0) if epx is not None else state\n",
    "        \n",
    "        if verbose: \n",
    "            print(\"y-prob: \", dlogp)\n",
    "            print(\"Stacked y-prob: \", epdlogp.size())    \n",
    "            print(\"Stacked h: \",eph.size())    \n",
    "            print(\"Stacked x: \",epx.size()) \n",
    "            \n",
    "        \"\"\"\n",
    "        Advance one step in the Pong env using action sampled \n",
    "        (Exit Torch(GPU/CPU) Land. Now in Numpy-CPU Land!!!)\n",
    "        \"\"\"\n",
    "        \n",
    "        action = sample.cpu().numpy() + 2   # action: Tensor --> numpy\n",
    "        \n",
    "        # step env through the sampled action\n",
    "        state, reward, done, _ = env.step(action)  # UP=2, DOWN=3\n",
    "        reward_sum += reward\n",
    "        if render:\n",
    "            env.render() \n",
    "            \n",
    "        \"\"\"\n",
    "        Stack rewards for policy update at the end of episode/batch\n",
    "        \"\"\"\n",
    "        rewards.append(reward)     \n",
    "        if verbose: \n",
    "            print(\"Pong action: \",action)\n",
    "            print(\"Stacked rewards: \",rewards) \n",
    "        \n",
    "        # Time to update policy - episode done or game steps exceed a preset value\n",
    "        if done or t > update_interval:\n",
    "            break\n",
    "            \n",
    "    \"\"\" \n",
    "    In REINFORCE, policy gradient is computed at the end of a batch/episode and then \n",
    "    used to update the Policy's weights\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print (\"We have finished a batch/episode. We now perform a Policy Update. \\n\")  \n",
    "    \n",
    "    \"\"\"\n",
    "    While the agent is playing, reward for each time step is stored in rewards[].\n",
    "    At the end of the episode, this is used to generate Q(s,a) for each time step.\n",
    "    (Exit Numpy-CPU Land. Now in Torch(GPU/CPU) Land!!!)\n",
    "    \"\"\"\n",
    "    \n",
    "    Q = estimate_Q(rewards, gamma)\n",
    "    epq = torch.from_numpy(Q).float().view(-1,1)  # reshape to Nx1 where N = # play steps\n",
    "   \n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    epq = (epq - epq.mean()) / (epq.std() + np.finfo(np.float32).eps)\n",
    "    \n",
    "    epdlogp *= epq  # modulate the gradient with advantage (PG magic happens right here.) \n",
    "    \n",
    "    if verbose:\n",
    "        print (\"Stacked Q(s,a): \",epq) \n",
    "        print (\"Stacked Q(s,a)*(y-prob): \", epdlogp) \n",
    "\n",
    "    # 2-layer Neural Network - Backward Pass\n",
    "    #  Input - epdlogp\n",
    "    #  Output - dW1, dW2\n",
    "\n",
    "    # Stage 3 - BackProp Matrix Multiplication                               #[3]\n",
    "    # logp = h_relu.mm(W2)\n",
    "    dW2 = torch.transpose(eph, 0, 1).mm(epdlogp)\n",
    "    dh = epdlogp.mm(torch.transpose(W2,0,1))\n",
    "    \n",
    "    # Stage 2 - Backprop ReLU                                                #[2]    \n",
    "    # h_relu = h.clamp(min=0)\n",
    "    dh[eph <= 0] = 0 \n",
    "\n",
    "    # Stage 1 - Matrix Multiplication                               #[1]    \n",
    "    # h = state.mm(W1)\n",
    "    dW1 = torch.transpose(epx, 0, 1).mm(dh)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"dW2: \",dW2.size())\n",
    "        print(\"dh_relu: \",dh.size())\n",
    "        print(\"dh: \",dh.size())\n",
    "        print(\"dW1: \",dW1.size())\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    W1 += lr * dW1\n",
    "    W2 += lr * dW2\n",
    "\n",
    "    # Keep track of running reward\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    reinforce.append([reward_sum, running_reward])\n",
    "    \n",
    "    # print out and show sign of life\n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast Reward: {:.2f}\\tRunning Reward: {:.2f}'.format(\n",
    "                i_episode, reward_sum, running_reward))\n",
    "    \n",
    "    # save model every 4000 episodes\n",
    "    if i_episode % 4000 == 0:\n",
    "        model = {}\n",
    "        model['W1'] = W1\n",
    "        model['W2'] = W2\n",
    "        pickle.dump(model, open('save.p', 'wb'))\n",
    "\n",
    "end = time.time()\n",
    "print (\"Time taken:\", start-end)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5.0413e+01  3.2784e+01 -9.9222e+01  ...   2.6483e+01 -5.6265e+01  3.5393e+01\n",
      " 4.4830e+01  5.7441e+00  5.6093e+01  ...  -1.1380e+02 -8.0942e+00  2.1449e+01\n",
      "-1.1022e+02 -4.1920e+01 -2.0435e+01  ...  -1.3575e+01  1.0510e+02 -1.0994e+02\n",
      "                ...                   ⋱                   ...                \n",
      " 2.6555e+01 -5.2463e+01  1.0094e+01  ...  -1.9175e+01 -4.2002e+01 -1.8343e+02\n",
      "-4.5048e+01 -2.6237e+00  1.9278e+01  ...  -3.2203e+01  1.5519e+02 -6.5791e+01\n",
      "-6.2598e+01 -3.2826e+01 -3.0377e-01  ...  -3.1031e+01  2.4677e+01 -2.5963e+01\n",
      "[torch.FloatTensor of size 6400x200]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "\n",
      " 0.3938  1.0000\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 2).scatter_(1,torch.LongTensor(sample) ,1)\n",
    "print (sample)\n",
    "print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.]\n",
      "\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.array([2.0])\n",
    "print (x)\n",
    "\n",
    "y = torch.from_numpy(x).float()\n",
    "print (y)\n",
    "\n",
    "y = torch.cat((y,y),0)\n",
    "\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.1776e+01 -5.4662e+01 -6.4076e+01  ...   2.2455e+00  8.0946e+00  4.8980e+01\n",
      "-3.8046e+01 -9.9706e+01 -1.6747e+00  ...  -6.9307e+00  5.2662e+00  6.4510e+01\n",
      "-3.3685e+01 -1.4248e+01  5.1007e+01  ...   1.3964e+02 -2.8429e+01 -1.7248e+01\n",
      "                ...                   ⋱                   ...                \n",
      " 1.6935e+01 -6.1469e+01  2.1259e+01  ...   4.1310e+01 -3.9917e+01 -6.9136e+01\n",
      " 1.0329e+01 -2.1218e+01 -4.6218e+01  ...  -1.9701e+01  4.9976e+01  1.7643e+01\n",
      "-7.0451e+01 -3.4414e+00  1.3894e-01  ...   8.3061e+01  1.5964e+01 -2.4826e+00\n",
      "[torch.FloatTensor of size 6400x200]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open('save.p', 'rb'))\n",
    "\n",
    "print (model['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
